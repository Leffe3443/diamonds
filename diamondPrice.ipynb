{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zXaqe1Pm4jQ"
      },
      "source": [
        "# About Dataset\n",
        "\n",
        "\n",
        "**price**: price in US dollars (\\$326--\\$18,823)\n",
        "\n",
        "**carat**: weight of the diamond (0.2--5.01)\n",
        "\n",
        "**cut**: quality of the cut (Fair, Good, Very Good, Premium, Ideal)\n",
        "\n",
        "**color**: diamond colour, from J (worst) to D (best)\n",
        "\n",
        "**clarity**: a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\n",
        "\n",
        "**x**: length in mm (0--10.74)\n",
        "\n",
        "**y**: width in mm (0--58.9)\n",
        "\n",
        "**z**: depth in mm (0--31.8)\n",
        "\n",
        "**depth**: total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43--79)\n",
        "\n",
        "**table**: width of top of diamond relative to widest point (43--95)\n",
        "\n",
        "**Link to dataset:**\n",
        "\n",
        "https://www.kaggle.com/datasets/shivam2503/diamonds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxAYYOodnd9C"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YZvuzVD9mvyg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f32c0094-316e-4d00-ed4e-d53051504e5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import lightgbm as lgb\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wDiXKhDniUU"
      },
      "source": [
        "# Load, examine and prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InDgeM_6nh4r"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('diamonds.csv')\n",
        "df.reset_index(drop=True)\n",
        "# df = df.dropna()\n",
        "# df = df.drop(df.iloc[:, :0], axis=1)\n",
        "\n",
        "\n",
        "df.head()\n",
        "# df.tail()\n",
        "# df.describe()\n",
        "# df.info()\n",
        "# df.shape\n",
        "\n",
        "# df.columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJebVFK4qjKR"
      },
      "outputs": [],
      "source": [
        "num_cols = len(df.columns)\n",
        "num_rows = math.ceil(num_cols / 2)  # 2 histograms per row (adjustable)\n",
        "\n",
        "# Create subplots with flexible rows and columns\n",
        "fig, axes = plt.subplots(num_rows, 2, figsize=(12, num_rows * 4))  # Dynamically adjust height\n",
        "axes = axes.flatten()  # Flatten the grid for easy iteration\n",
        "\n",
        "# Plot histograms\n",
        "for i, column in enumerate(df.columns):\n",
        "    df[column].hist(grid=False, edgecolor='black', ax=axes[i])\n",
        "    axes[i].set_title(f'Distribution of {column}')\n",
        "    axes[i].set_xlabel(f'{column} Values')\n",
        "    axes[i].set_ylabel('Frequency')\n",
        "\n",
        "# Hide unused subplots\n",
        "for i in range(len(df.columns), len(axes)):\n",
        "    axes[i].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gC62gt-qV3i"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3wQEpf7vQ-i",
        "outputId": "46757037-e03d-45d2-c881-7c45634b4ff5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005051 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1804\n",
            "[LightGBM] [Info] Number of data points in the train set: 42073, number of used features: 25\n",
            "[LightGBM] [Info] Start training from score 3942.695101\n"
          ]
        }
      ],
      "source": [
        "X = df.drop('price', axis=1)\n",
        "y = df['price']\n",
        "\n",
        "\n",
        "# One-hot encode cut, color and clarity\n",
        "X = pd.get_dummies(X, columns=[\"cut\", \"color\", \"clarity\"], drop_first=True)\n",
        "\n",
        "X.columns = X.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)\n",
        "\n",
        "# X.drop(\"Unnamed_0\", axis=1, inplace=True)\n",
        "\n",
        "# X.columns\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.22, random_state=42)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "# scaler = MinMaxScaler()\n",
        "\n",
        "for data in [X_train, X_test]:\n",
        "    data['volume'] = data['x'] * data['y'] * data['z']\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train[['carat', 'depth', 'table', 'volume']] = scaler.fit_transform(\n",
        "    X_train[['carat', 'depth', 'table', 'volume']]\n",
        ")\n",
        "X_test[['carat', 'depth', 'table', 'volume']] = scaler.transform(\n",
        "    X_test[['carat', 'depth', 'table', 'volume']]\n",
        ")\n",
        "\n",
        "# LGBM Regressor\n",
        "\n",
        "X_train_lgmb = scaler.fit_transform(X_train)\n",
        "X_test_lgmb = scaler.transform(X_test)\n",
        "\n",
        "lgbm = lgb.LGBMRegressor()\n",
        "lgbm.fit(X_train, y_train)\n",
        "\n",
        "y_pred_lgbm = lgbm.predict(X_test)\n",
        "\n",
        "# Linear Regression\n",
        "\n",
        "X_train_lr = scaler.fit_transform(X_train)\n",
        "X_test_lr = scaler.transform(X_test)\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "y_pred_lr = lr.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of LGBM\n",
        "mae = metrics.mean_absolute_error(y_test, y_pred_lgbm)\n",
        "mse = metrics.mean_squared_error(y_test, y_pred_lgbm)\n",
        "rmse = metrics.mean_squared_error(y_test, y_pred_lgbm)\n",
        "\n",
        "\n",
        "print(\"LGBM regressor:\")\n",
        "print(\"\")\n",
        "print(f'MAE: {mae}')\n",
        "print(f'MSE: {mse}')\n",
        "print(f'RMSE: {rmse}')\n",
        "\n",
        "# Get R-squared score for the test set\n",
        "score_train = lgbm.score(X_train, y_train)\n",
        "print(f'Train score: {score_train:.5f}')\n",
        "\n",
        "\n",
        "score_test = lgbm.score(X_test, y_test)\n",
        "print(f'Test score: {score_test:.5f}')\n",
        "\n",
        "print(\"\"\"____\"\"\")\n",
        "print(\"\")\n",
        "# Evaluation of Linear Regression\n",
        "print(\"Linear Regression\")\n",
        "\n",
        "mae = metrics.mean_absolute_error(y_test, y_pred_lr)\n",
        "mse = metrics.mean_squared_error(y_test, y_pred_lr)\n",
        "rmse = metrics.mean_squared_error(y_test, y_pred_lr)\n",
        "\n",
        "print(f'MAE: {mae}')\n",
        "print(f'MSE: {mse}')\n",
        "print(f'RMSE: {rmse}')\n",
        "\n",
        "# Get R-squared score for the test set\n",
        "score_train = lr.score(X_train, y_train)\n",
        "print(f'Train score: {score_train:.5f}')\n",
        "\n",
        "\n",
        "score_test = lr.score(X_test, y_test)\n",
        "print(f'Test score: {score_test:.5f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9Ur4DnIrAiE",
        "outputId": "ca48f12c-e48a-44c1-908d-d925c814d9da"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LGBM regressor:\n",
            "\n",
            "MAE: 30.512075361175544\n",
            "MSE: 3686.878496998431\n",
            "RMSE: 3686.878496998431\n",
            "Train score: 0.99985\n",
            "Test score: 0.99977\n",
            "____\n",
            "\n",
            "Linear Regression\n",
            "MAE: 730.863395499346\n",
            "MSE: 1259032.126197795\n",
            "RMSE: 1259032.126197795\n",
            "Train score: 0.92055\n",
            "Test score: 0.92029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural network"
      ],
      "metadata": {
        "id": "GtYSyif87nzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.activations import relu\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Z3JkGBljFZZw"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and prepare data"
      ],
      "metadata": {
        "id": "s10fCRGYFeze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "data = pd.read_csv('diamonds.csv')\n",
        "\n",
        "# Preprocessing\n",
        "X = data.drop(columns=['price'])\n",
        "y = data['price']\n",
        "\n",
        "# Feature engineering, create volume\n",
        "X['volume'] = X['x'] * X['y'] * X['z']\n",
        "\n",
        "# One-hot encoding\n",
        "X = pd.get_dummies(X, columns=['cut', 'color', 'clarity'], drop_first=True)"
      ],
      "metadata": {
        "id": "9OZ5SlBLFbjc"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and test NN"
      ],
      "metadata": {
        "id": "m-_zhluxFhnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.22, random_state=42)\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Functional API Model\n",
        "input_layer = Input(shape=(X_train.shape[1],))  # Input layer\n",
        "\n",
        "# First hidden layer\n",
        "x = Dense(128)(input_layer)\n",
        "x = BatchNormalization()(x)\n",
        "x = tf.keras.activations.relu(x)\n",
        "x = Dropout(0.3)(x)\n",
        "\n",
        "# Second hidden layer\n",
        "x = Dense(64)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = tf.keras.activations.relu(x)\n",
        "x = Dropout(0.3)(x)\n",
        "\n",
        "output_layer = Dense(1, activation='linear')(x)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Callback to reduce learning rate\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=5,\n",
        "    min_lr=1e-6\n",
        ")\n",
        "\n",
        "# Training the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    callbacks=[reduce_lr]\n",
        ")\n",
        "\n",
        "# Evaluate on test data\n",
        "\n",
        "\n",
        "test_loss, test_mae = model.evaluate(X_test, y_test, verbose=2)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test MAE: {test_mae}\")\n",
        "# print(f\"Test R^2: {r2}\")"
      ],
      "metadata": {
        "id": "bckFtHqo7nc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# Save model architecture to an image file\n",
        "plot_model(model, to_file='model_architecture.png', show_shapes=True)"
      ],
      "metadata": {
        "id": "0D_dL6muGPne"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}